# Research APIs Integration Guide

## Overview

The Cogito platform now includes comprehensive research capabilities that go beyond ArXiv, integrating multiple research databases and web search to provide broader coverage across scientific disciplines.

## Supported Research Sources

### 1. PubMed (Biomedical Research)
- **Database**: NCBI PubMed with 35+ million citations
- **Coverage**: Biomedical and life sciences literature
- **API**: NCBI E-utilities (free, API key optional)
- **Configuration**: `config.json → research_apis → pubmed`

### 2. Semantic Scholar (Computer Science)
- **Database**: 200+ million papers across disciplines
- **Coverage**: Computer science, biology, neuroscience, and more
- **API**: Semantic Scholar API (free, API key optional for higher limits)
- **Configuration**: `config.json → research_apis → semantic_scholar`

### 3. CrossRef (DOI Resolution)
- **Database**: 130+ million scholarly works
- **Coverage**: All disciplines via DOI metadata
- **API**: CrossRef REST API (free, polite rate limits)
- **Configuration**: `config.json → research_apis → crossref`

### 4. Web Search
- **Providers**: SerpAPI (Google/Bing) with DuckDuckGo fallback
- **Coverage**: General web, blogs, news, documentation
- **API**: SerpAPI (requires key) or DuckDuckGo (free HTML scraping)
- **Configuration**: `config.json → research_apis → web_search`

## Installation

The research APIs are included by default. Ensure dependencies are installed:

```bash
pip install -r requirements.txt
```

This includes:
- `requests` - HTTP client
- `beautifulsoup4` - HTML parsing for web search

## Configuration

Edit `config.json` to configure research APIs:

```json
{
  "research_apis": {
    "pubmed": {
      "enabled": true,
      "api_key": "",
      "timeout": 30,
      "max_retries": 3,
      "tool_name": "Cogito",
      "email": "your-email@example.com"
    },
    "semantic_scholar": {
      "enabled": true,
      "api_key": "",
      "timeout": 30,
      "max_retries": 3
    },
    "crossref": {
      "enabled": true,
      "timeout": 30,
      "max_retries": 3,
      "email": "your-email@example.com"
    },
    "web_search": {
      "enabled": true,
      "api_key": "",
      "timeout": 30,
      "search_engine": "duckduckgo",
      "use_fallback": true
    }
  }
}
```

### API Keys (Optional)

While all research sources work without API keys, obtaining keys provides:
- Higher rate limits
- Better performance
- Enhanced features

**PubMed**: Register at https://www.ncbi.nlm.nih.gov/account/  
**Semantic Scholar**: Request at https://www.semanticscholar.org/product/api  
**SerpAPI**: Sign up at https://serpapi.com/

Add keys to environment variables or `config.json`:

```bash
export PUBMED_API_KEY="your-key"
export SEMANTIC_SCHOLAR_API_KEY="your-key"
export SERPAPI_KEY="your-key"
```

## Usage

### Command-Line Interface

#### List Available Sources

```bash
python run_research.py list-sources
```

Output:
```
Available research sources:
  - pubmed
  - semantic_scholar
  - crossref
  - web_search
```

#### Run Ad-Hoc Query

```bash
python run_research.py query "quantum computing applications" \
  --max-results 10 \
  --sources pubmed,semantic_scholar \
  --output results.json
```

#### Execute Query Plan

Query plans are generated by the preflight system:

```bash
# First, extract insights and build queries from input
python run_critique.py input/documents/ \
  --preflight-extract \
  --preflight-build-queries \
  --points-out artifacts/points.json \
  --queries-out artifacts/queries.json

# Then execute the research queries
python run_research.py execute-plan artifacts/queries.json \
  --output artifacts/research_results.json \
  --max-results 10
```

### Python API

```python
from src.research_apis import ResearchAPIOrchestrator
from src.application.research_execution import ResearchQueryExecutor

# Initialize orchestrator with configuration
config = {
    'research_apis': {
        'pubmed': {'enabled': True},
        'semantic_scholar': {'enabled': True},
    }
}
orchestrator = ResearchAPIOrchestrator(config=config)

# Search across all enabled sources
results = orchestrator.search_all(
    query="machine learning healthcare",
    max_results_per_source=10,
    parallel=True
)

# Print results
for result in results:
    print(f"{result.title} ({result.source})")
    print(f"  {result.url}")
    print()
```

## Integration with Critique Pipeline

The research APIs integrate seamlessly with Cogito's critique pipeline:

### 1. Directory Input → Preflight Extraction

```bash
python run_critique.py --input-dir ./research_notes \
  --preflight-extract \
  --points-out artifacts/points.json
```

This extracts key insights from your documents.

### 2. Preflight Query Building

```bash
python run_critique.py --input-dir ./research_notes \
  --preflight-extract \
  --preflight-build-queries \
  --points-out artifacts/points.json \
  --queries-out artifacts/queries.json \
  --max-queries 8
```

This generates research queries from the extracted insights.

### 3. Query Execution

```bash
python run_research.py execute-plan artifacts/queries.json \
  --output artifacts/research_results.json \
  --sources pubmed,semantic_scholar,web_search
```

This executes the queries across research databases.

### 4. Use Results in Critique

The research results can be referenced during critique:

```bash
python run_critique.py --input-dir ./research_notes \
  --scientific \
  --peer-review
```

## Architecture

### Clean Architecture Layers

```
Presentation Layer (CLI)
├── run_research.py
└── src/presentation/cli/research_cli.py

Application Layer (Business Logic)
└── src/application/research_execution/
    └── services.py (ResearchQueryExecutor)

Domain Layer (Models)
└── src/domain/preflight/models.py
    ├── QueryPlan
    └── BuiltQuery

Infrastructure Layer (External APIs)
└── src/research_apis/
    ├── base.py (ResearchAPIBase, ResearchResult)
    ├── pubmed.py (PubMedAPI)
    ├── semantic_scholar.py (SemanticScholarAPI)
    ├── crossref.py (CrossRefAPI)
    ├── web_search.py (WebSearchAPI)
    └── orchestrator.py (ResearchAPIOrchestrator)
```

### Key Design Principles

1. **Modular**: Each research source is independent
2. **Extensible**: Easy to add new sources by inheriting from `ResearchAPIBase`
3. **Resilient**: Automatic retries, fallbacks, graceful degradation
4. **Observable**: Comprehensive logging at all levels
5. **Configurable**: All behavior controlled via config

## Adding New Research Sources

To add a new research source:

1. Create a new file in `src/research_apis/`:

```python
from .base import ResearchAPIBase, ResearchResult

class NewSourceAPI(ResearchAPIBase):
    def _setup_client(self):
        # Initialize your HTTP client
        pass
    
    def search(self, query, max_results=10, filters=None):
        # Implement search logic
        pass
    
    def get_by_id(self, item_id):
        # Implement ID lookup
        pass
    
    @property
    def source_name(self):
        return "New Source"
```

2. Register in `src/research_apis/__init__.py`:

```python
from .new_source import NewSourceAPI

__all__ = [..., 'NewSourceAPI']
```

3. Add to orchestrator in `src/research_apis/orchestrator.py`:

```python
if research_config.get('new_source', {}).get('enabled', True):
    self.providers['new_source'] = NewSourceAPI(
        api_key=research_config.get('new_source', {}).get('api_key'),
        config=research_config.get('new_source', {})
    )
```

4. Add configuration to `config.json`.

## Troubleshooting

### No Results Returned

- Check network connectivity
- Verify API keys if using authenticated endpoints
- Check logs for error messages
- Try with `--sequential` flag instead of parallel

### Rate Limit Errors

- Add API keys to increase limits
- Reduce `max_results` parameter
- Add delay between queries (set `retry_delay` in config)
- Use fewer sources simultaneously

### Timeout Errors

- Increase `timeout` in config
- Use `--sequential` for more reliable execution
- Check network latency

### Import Errors

```bash
# Reinstall dependencies
pip install -r requirements.txt
```

## Performance Considerations

### Parallel vs Sequential Execution

- **Parallel** (default): Faster, queries all sources simultaneously
- **Sequential** (`--sequential`): Slower but more reliable, queries one at a time

### Result Limits

Adjust `max_results` based on needs:
- Small queries: 5-10 results per source
- Comprehensive research: 20-50 results per source
- Exhaustive search: 50-100 results per source (may hit rate limits)

### Caching

Currently, results are not cached. Future enhancements will add:
- Local result caching
- Deduplicated query execution
- Persistent cache database

## Future Enhancements

Planned improvements:
- IEEE Xplore integration
- Google Scholar integration (via SerpAPI)
- Result relevance ranking
- Persistent result caching
- Query template library
- Automatic query refinement
- Citation graph traversal

## Support

For issues or questions:
1. Check logs in `logs/system.log`
2. Review error messages for specific provider failures
3. Test with `list-sources` to verify initialization
4. Open an issue on GitHub

## License

This integration follows the same MIT License as the Cogito project.
